\section*{Text Classification}
\label{sec: textClass}

Classification is a supervised data mining technique that involves assigning a label to a set of unlabeled input objects. In this work, we have implemented a binary classification that classifies input objects into one of the two classes.\\
\\
In text classification, algorithms are applied to text documents. The goal is to assign a document in one class based on its content. In order to train a recommendation system, the input documents have been divided into train and testing data. The training set contains all the labeled documents. Testing data sets are those where the documents are unlabeled. The system learns the model from the labeled data and predicts the class label on the testing documents. This type of learning is called supervised learning because a supervisor defines  the  classes  and  labels  training  documents. 
The trained recommendation system applies a classification function $f$ that maps the document $D$ to class $C$.\\
The dimension of the training and testing data set is very important. If the classifier is fed with a small number of documents to train from, it may not acquire substantial knowledge to classify the test data correctly. If the training data is too large compared to the test data, it leads to overfitting.  Even the ratio of positive to negative instances plays a crucial role during the training step.

\subsection*{Text Representation}

In order to train the model and compute the classification function, the input set has to be studied and translated in feature sets. Hence, there is a preliminary work to perform on the data set to represent text in a structured manner.\\
The most common technique to represent text is the Bag of Words model. Note that in this model the order of words is ignored, bigrams or n-grams could be considered to keep track of the spacial property of the words in a sentence.\\
From the id of the events clicked, a MySQL query retrieves the details of the object. The title, the category and the genre have been added and linked to the action. Other experiments have been carried out considering additional details such as the description, the director, actors, and the location. 

Each records has been converted into a string containing a combination of the title, the category and the genre. Then, the class label is appended to the string.

\subsection*{Feature Vectors}
The preprocess step brakes the record down into tokens or individual words. This process is also referred as Tokenization and each word represents a feature. For each document a vector of features is determined.\\
Italian and english stopwords have been removed from the temporary feature sets and features have been grouped by their roots (stemming).
\\
Then all words have been weighted using the their Term Frequency in the training set i.e. the total number of occurrences of a particular word in a collection of documents.\\ 
Finally, the feature vector is filled with the $N$\footnote{Several percentages of features to consider in the final vector have been tested. The threshold depends on the size of the training set. In our case the size of the feature vector is 1140 words.} most common words in the training set.

\subsection*{Feature reduction}
Feature reduction was an important part of the optimization of the classifiers' performance. Reducing the feature vector to a size that does not exceed the number of training cases was a starting point.\\
\\
Since the dimension of the feature vector increases with length of the document. Reducing the number of features can be done in several ways:
\begin{itemize}
	\item reduction to the top n features
	\item reduction by elimination of sets of features
	\item reduction by combining the above methods
\end{itemize} 
The proposed approach is to eliminate a subset of features and then take the top ranking n features from the remaining set.\\
Techniques like stop word removal and stemming are commonly applied. Stop word removal involves the elimination of words which add no significant value to the  document. Stemming is the process of reducing derived words to their word stem, base or root form.
All the above have been applied using the StringToWordVector function in Weka and implemented in Python.